{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-02-03\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Maria Johansson (marjo123), Erik Karlsson (erika456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will experiment with $n$-gram models. You will test various parameters that influence these models&rsquo; quality and train to estimate models with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code import the Python modules needed for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lab2\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab consists of Arthur Conan Doyle&rsquo;s novels about Sherlock Holmes: *The Adventures of Sherlock Holmes*, *The Memoirs of Sherlock Holmes*, *The Return of Sherlock Holmes*, *His Last Bow* and *The Case-Book of Sherlock Holmes*. The next piece of code loads the first three of these as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = lab2.read_data(\"/home/729G17/labs/lab2/data/advs.txt\",\n",
    "                               \"/home/729G17/labs/lab2/data/mems.txt\",\n",
    "                               \"/home/729G17/labs/lab2/data/retn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as a list of sentences, where one sentence is represented as a list of tokens (strings). The next line prints the 101th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'Let', 'us', 'glance', 'at', 'our', 'Continental', 'Gazetteer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and its order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this lab you will examine the relation between an $n$-gram model’s quality and its **order**, i.e. the value of&nbsp;$n$. You will do both a qualitative and quantitative evaluation with the help of the entropy measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line trains a bigram-model of the class `ngrams.Model` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lab2.train(ngrams.Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model you are able to generate random sentences. Every time you run the following code cell a new sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sometimes I am sure that we will trouble you keep the third from a quarter of another cigarette .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(model.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sentences. Do they sound natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a unigram-, bigram-, trigram-, and quadrigram-model, and generate random sentences with each. How does the quality of the sentences change with the model’s order? Explain your observations using your understanding from how an $n$-gram model works. Use some generated sentences in order to illustrate your discussion. How would the sentences look like for higher values of $n$, such as $n=10$?\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ill-gotten . . lady's me we\n",
      "One was a little disturbed and being insane from death .\n",
      "An hour and half an hour the miserable weather and the Colonel for five years and was not in a sort of three-card trick , is my own mind as to make it clear ?\n",
      "Holmes sat down and stretched his legs before the fire and composed himself to listen .\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "uni = lab2.train(ngrams.Model, 1, training_data)\n",
    "bi = lab2.train(ngrams.Model, 2, training_data)\n",
    "tri = lab2.train(ngrams.Model, 3, training_data)\n",
    "quad = lab2.train(ngrams.Model, 4, training_data)\n",
    "\n",
    "print(\" \".join(uni.generate()))\n",
    "print(\" \".join(bi.generate()))\n",
    "print(\" \".join(tri.generate()))\n",
    "print(\" \".join(quad.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert your discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do a quantitative evaluation of a model we can compute its **entropy** on held-out data. We will use the first part of the novel *The Adventures of Sherlock Holmes* for this. It is loaded by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = lab2.read_data(\"/home/729G17/labs/lab2/data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model and computes its entropy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.426862596420277"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lab2.train(ngrams.Model, 2, training_data)\n",
    "lab2.evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Compute the entropy for the four models you created for the previous problem. How does the model’s entropy change with the model’s order? Explain using your knowledge of the entropy measure.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.337551182974018\n",
      "3.426862596420277\n",
      "1.4289533769461726\n",
      "0.5436027106964166\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "print(lab2.evaluate(uni, test_data))\n",
    "print(lab2.evaluate(bi, test_data))\n",
    "print(lab2.evaluate(tri, test_data))\n",
    "print(lab2.evaluate(quad, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert your discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and the estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab you will implement and evaluate various estimation methods. In order to do that you will need to know how the lab system is built up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The content of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the `train()` function (like you did above), the system creates an $n$-gram model of the given class (so far: `ngrams.Model`) and with the given order (the value of $n$) and trains the model on the given data set. For the second part of this lab you will use your own model class. We start with defining the class in such a way that it simply calls the corresponding methods of the superclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def order(self):\n",
    "        \"\"\"Return the order of this model (an integer).\"\"\"\n",
    "        return super().order()\n",
    "    \n",
    "    def vocabulary(self):\n",
    "        \"\"\"Return this model's vocabulary (a set).\"\"\"\n",
    "        return super().vocabulary()\n",
    "    \n",
    "    def freq(self, ctxt, word):\n",
    "        \"\"\"Return the number of occurrences of `word` (a string) after `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().freq(ctxt, word)\n",
    "    \n",
    "    def total(self, ctxt):\n",
    "        \"\"\"Return the total number of ngrams that start with `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().total(ctxt)\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().prob(ctxt, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model of the class `Model` and prints the model’s order (an integer) and the size of its vocabulary (a set of strings, represented by Python’s `set` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of the model: 2\n",
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "model = lab2.train(Model, 2, training_data)\n",
    "print(\"order of the model:\", model.order())\n",
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up an n-gram’s absolute frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trained model consists primarily of a table with absolute frequencies for all $n$-grams that appear in the text it was trained on. In order to look up an $n$-gram’s absolute frequency one can use the method `freq()`. An $n$-gram is divided into two parts: an $(n-1)$-gram called **context** (`ctxt`) and a final unigram (`word`). In Python the context is represented as a tuple of strings and the unigram as a normal string.\n",
    "\n",
    "If you want to train a trigram model and then know the absolute frequency for the trigram *Mr. Sherlock Holmes* you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lab2.train(Model, 3, training_data)\n",
    "model.freq((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a bigram model and looking up the absolute frequency for the bigram *Baker Street* you can write the following. Note that the context of a bigram model is a 1-tuple of strings, which has a special notation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lab2.train(Model, 2, training_data)\n",
    "model.freq((\"Baker\",), \"Street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the absolute frequency of an n-gram with a given context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `total()` returns the absolute frequency of $n$-grams with the given kontext. Here is an example for a trigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lab2.train(Model, 3, training_data)\n",
    "model.total((\"Mr.\", \"Sherlock\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a bigram model and use it to calculate the following values, using the methods shown above.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lab2.train(Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** the absolute frequency for the bigram *Sherlock Holmes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "total() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6ed5daddfc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Insert your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mr.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sherlock\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: total() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "model.total((\"Mr.\"), \"Sherlock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.** the absolute frequency of bigrams with the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.** the absolute frequency for the unigram *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.** the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.** a list with all the words following the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For the last exercise you will need to write a bit more than a simple function call.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with the Maximum Likelihood method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `prob()` returns the estimated conditional probability $P(w|c)$ for a word $w$ given a context $c$. The following code snippet trains a trigram model and estimates the pobability for *Holmes* given the context *Mr. Sherlock*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = lab2.train(Model, 3, training_data)\n",
    "model.prob((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(What does the returned value imply?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Do your own implementation of the method `prob()`. The method should estimate probabilities using the Maximum Likelihood method. Test your implementation by redoing Exercise&nbsp;2 with the new class; you should get the same result as before. Use the code you wrote in Exercise&nbsp;3 in order to solve the exercise.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        # TODO: Replace the next line with your own code\n",
    "        return super().prob(ctxt, word)\n",
    "\n",
    "# TODO: Insert your testing code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this exercise you will need to turn the formula for Maximum Likelihood estimation into code. We illustrate the formula for a bigram model. If we write $f(w_1w_2)$ for the number of occurrences of the bigram  $w_1w_2$ and $f(w_1)$ for the number of occurrences of the unigram $w_1$, then the probability for observing $w_2$ given $w_1$ is\n",
    "$$\n",
    "P(w_2|w_1) = \\frac{f(w_1w_2)}{f(w_1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Maximum Likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `yoda.txt` contains the same text as `test.txt`, but in the jumbled [Yoda-language]( http://itre.cis.upenn.edu/~myl/languagelog/archives/002173.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yoda_data = lab2.read_data(\"/home/729G17/labs/lab2/data/yoda.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 5</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation of the four previous models with `yoda.txt` as test data. Something unexpected happens for models with $n>1$. Why? Explain the problem with your knowledge of Maximum Likelihood estimation.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert your discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next problem you are going to do Maximum Likelihood estimation, but with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 6</div>\n",
    "<div class=\"panel-body\">\n",
    "<p>\n",
    "Write a new implementation of the method `prob()`, such that it estimates probabilities with additive smoothing.</p>\n",
    "<p>\n",
    "Evaluate the system with new new class using the entropy measure from Problem&nbsp;2. Choose the following values for the smoothing constant $k$:&nbsp;0,00,&nbsp;1,00, 0,10, 0,01. For $k=0$ you should get the same result as in Problem&nbsp;5.\n",
    "</p>\n",
    "<p>\n",
    "How and why does the smoothing constant influence the model’s entropy? Connect to the distribution of the probability mass between observed and imaginary occurrences.\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        # TODO: Replace the next line with your own code\n",
    "        return super().prob(ctxt, word)\n",
    "\n",
    "# TODO: Insert your testing code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the table below with your entropy measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td></td><td>k = 0,00</td><td>k = 1,00</td><td>k = 0,10</td><td>k = 0,01</td></tr>\n",
    "<tr><td>n = 1</td><td>to fill</td><td>to fill</td><td>to fill</td><td>to fill</td></tr>\n",
    "<tr><td>n = 2</td><td>to fill</td><td>to fill</td><td>to fill</td><td>to fill</td></tr>\n",
    "<tr><td>n = 3</td><td>to fill</td><td>to fill</td><td>to fill</td><td>to fill</td></tr>\n",
    "<tr><td>n = 4</td><td>to fill</td><td>to fill</td><td>to fill</td><td>to fill</td></tr>\n",
    "</table>\n",
    "\n",
    "*TODO: Insert your discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An unseen test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your last exercise is to redo the evaluation on a previously unseen test set, texts from the collection *His Last Bow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unseen_data = lab2.read_data(\"/home/729G17/labs/lab2/data/lstb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 7</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation from Problem 6 with the new test data. Explain what happens given the differences between `test.txt` and `lstb.txt`.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Insert your explanations here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
