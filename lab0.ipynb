{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2017-01-20\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0: Text Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students:** Maria Johansson (marjo123), Erik Karlsson (erika456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a computer's perspective, a text primarily is a sequence of characters, such as letters and digits. Before we can process a text with language technology tools, we need to segment it into linguistically more meaningful units, such as paragraphs, sentences, or words. This basic technique is called **text segmentation**. When the target units are words, it is called **tokenisation**. In this lab you will implement a simple tokeniser for continuous text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lab0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text you will be working with is an article from Swedish Wikipedia: [Gustav III](https://sv.wikipedia.org/wiki/Gustav_III). Look at the webpage and see how it is built up.\n",
    "\n",
    "A Wikipedia-page not only consists of text but even of other data, such as pictures and tables. Before you can start tokenising the text, you would usually need to extract it from the page using a tool like [Scrapy](https://scrapy.org). For this laboratory this has been already done for you, which means that your starting point is going to be the extracted text.\n",
    "\n",
    "### Read in the raw text\n",
    "\n",
    "In order to read in the extracted text in Python, we define a helper function `read_data()`. The function opens the given file and returns its content as a list with lines of text. The textfile uses newline characters (`\\n`) to end each line; this character is removed using Python's [`str.rstrip()`](https://docs.python.org/3.5/library/stdtypes.html#str.rstrip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        return [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now read in the raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1 = read_data(\"/home/TDP030/labs/lab0/data/text1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the text in a text editor and try to identify peculiarities that might create problems for further analysis. The text is automatically extracted, using methods that read the data from the website's HTML tree.\n",
    "\n",
    "You can even look at the text directly from the notebook. The next command prints a list with the first 50 lines of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text1[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet recreates the content from the text file in lines 51 to 60, glueing the lines together using the newline character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(text1[50:60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the gold standard\n",
    "\n",
    "There exists a gold standard tokenisation for the raw text. This tokenisation follows the rules used in the [Stockholm–Umeå Corpus (SUC)](https://spraakbanken.gu.se/swe/resurs/suc3), a standard corpus for Swedish. The file containing the gold standard tokenisation consists of all tokens from the raw text, with one token per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold1 = read_data(\"/home/TDP030/labs/lab0/data/gold1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the gold standard and try to understand the principles it is based on. Most tokens are normal words or punctuation marks, but note that abbreviations are handled as one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gold1[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitespace tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains a very simple tokeniser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_ws(lines):\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        for token in line.split():\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a list with text lines, splits every line at whitespace using the function [`str.split()`](https://docs.python.org/3.5/library/stdtypes.html#str.split), and collects the resulting strings in a list `tokens`.\n",
    "\n",
    "### Compare the tokenisation with the gold standard\n",
    "\n",
    "Test the tokeniser on the first 50 lines of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tokenize_ws(text1[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this tokenisation with the gold standard. Which differences do you find?\n",
    "\n",
    "Most differences can be explained as **undersegmentation**, where the tokeniser has missed to split a token. The opposite is **oversegmentation**, where the tokeniser splits a character sequence that should really be one token.\n",
    "\n",
    "In order to examine the differences, you can use the function `diff()` from the lab module. This function expects two arguments, a list with gold standard tokens and a list with automatically predicted tokens. It returns a new list that shows the differences between the two tokenisations in a compact way. The following command shows the first ten differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lab0.diff(gold1, tokenize_ws(text1))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list contains pairs whose first component is a sequence of tokens that appear in the gold standard but not in the automatic tokenisation, and whose second component is a sequence of tokens that appear in the automatic tokenisation but not in the gold standard. The following code snippet prints the list in a more readable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper function that formats a list of tokens\n",
    "def fmt_tokens(tokens):\n",
    "    return \" \".join(tokens) + \" (%d)\" % len(tokens)\n",
    "\n",
    "# Print out information about divergent subsequences\n",
    "print(\"Gold tokens\".ljust(40), \"Predicted tokens\".ljust(40))\n",
    "print()\n",
    "for gold_tokens, pred_tokens in lab0.diff(gold1, tokenize_ws(text1)):\n",
    "    print(fmt_tokens(gold_tokens).ljust(40), fmt_tokens(pred_tokens).ljust(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Examine the differences between the gold standard and the whitespace-based tokenisation. Try to classify different types of undersegmentation and think of ways how one could eliminate them. Give at least three examples from different classes. Give at least one example of oversegmentation. In order to solve this problem, you can examine the output from the previous code cell by hand or write code to solve this task for you.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You might want to write some code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Room for your answer*\n",
    "\n",
    "Examples for different types of under-segmentation:\n",
    "\n",
    "* Example 1\n",
    "* Example 2\n",
    "* Example 3\n",
    "\n",
    "Example for over-segmentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute precision and recall\n",
    "\n",
    "A way to do a more quantitative evaluation of the tokeniser is to compute its **precision** and its **recall**. Precision is defined as the number of correct tokens among all tokens the system has identified. Recall is defined as the number of correctly identified tokens among all tokens in the gold standard. In order to compute those values you can use the next code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_ws = tokenize_ws(text1)\n",
    "\n",
    "print(\"Errors: %d\" % lab0.n_errors(gold1, tokens_ws))\n",
    "print(\"Precision: %.4f\" % lab0.precision(gold1, tokens_ws))\n",
    "print(\"Recall: %.4f\" % lab0.recall(gold1, tokens_ws))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation based on regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab you will exchange the simple whitespace-based tokenisation with a more advanced tokenisation based on **regular expressions**. Before you can use regular expressions in Python you have to first load the relevant module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple tokeniser based on regular expressions looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_re(regex, lines):\n",
    "    output = []\n",
    "    for line in lines:\n",
    "        for match in re.finditer(regex, line):\n",
    "            output.append(match.group(0))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function finds all longest, non-overlapping occurrences of the pattern `regex` in the row `line` and returns them as a list. The line is scanned from left to right and the matching substrings are returned in the same order.\n",
    "\n",
    "In order to simulate and run the whitespace-based tokeniser using regular expression you can use the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Regular expression the tokeniser will use\n",
    "regex = r'\\S+'\n",
    "\n",
    "tokens_re = tokenize_re(regex, text1)\n",
    "\n",
    "print(\"Errors: %d\" % lab0.n_errors(gold1, tokens_re))\n",
    "print(\"Precision: %.4f\" % lab0.precision(gold1, tokens_re))\n",
    "print(\"Recall: %.4f\" % lab0.recall(gold1, tokens_re))\n",
    "\n",
    "# In order to debug the regex, you might want to comment in the next line.\n",
    "# lab0.diff(gold1, tokens_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Find a regular expression that eliminates as many differences between the gold standard and the automatic tokenisation as possible. Your finished tokeniser should have at least 99.5% precision and recall.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some hints that can help you with the exercise:\n",
    "\n",
    "* Read [Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html) and [the documentation for the module  `re`](https://docs.python.org/3.5/library/re.html).\n",
    "\n",
    "* If you want to use grouping sub-expressions, you might want to use *non-capturing* groups.\n",
    "\n",
    "* If your expression gets too long and hard to read, have a look at [`re.VERBOSE`](https://docs.python.org/3.5/library/re.html#re.VERBOSE) for writing the expression over multiple lines.\n",
    "\n",
    "* If you want to practice your regex skills a little more, hop over to [RegexOne](https://regexone.com) or [RegExr](http://regexr.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the tokeniser on new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your last exercise is to evaluate your tokeniser on another article from Swedish Wikipedia: [Katarina II av Ryssland](https://sv.wikipedia.org/wiki/Katarina_II_av_Ryssland). (She was Gustav&nbsp;III's cousin.)\n",
    "\n",
    "The raw text and the gold standard tokenisation is loaded like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text2 = read_data(\"/home/TDP030/labs/lab0/data/text2.txt\")\n",
    "gold2 = read_data(\"/home/TDP030/labs/lab0/data/gold2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo Problem&nbsp;1 on the new text. Compute precision and recall as well. Report the results and try to explain them. Write a short text (max. 250 words) of discussion.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Room for your evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Room for your discussion (max. 250 words)*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
